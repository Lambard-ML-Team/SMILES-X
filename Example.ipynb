{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/SMILESX_logo.png\" alt=\"drawing\" width=\"300\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the SMILES-X tutorial!\n",
    "\n",
    "In this brief tutorial we cover the basics of the SMILES-X package.\n",
    "Here we show how to train, extract and use trained SMILES-X models. More detailed information on the package implementation and fundamentals can be found in the documentation.\n",
    "\n",
    "The main body of the SMILES-X is contained in the `main.py`, which we need to import.\\We will also need to import the library that is going to be used for data reading and preprocessing. Here we use `pandas` to work with the `./data/example_data.csv` data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from SMILESX import main\n",
    "\n",
    "%load_ext autoreload\n",
    "%aimport SMILESX\n",
    "%autoreload 1\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data folder and extension\n",
    "data_path = \"./data/example_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide data information\n",
    "data_name = 'Test'\n",
    "data_label = 'Test label' # will show on plots\n",
    "data_units = 'units' # will show on plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format\n",
    "\n",
    "The data format is not critical as long as it can be shaped into a numpy array, and provides the following information:\n",
    "- SMILES\n",
    "- ground true values (experimental or theoretical, mean or median)\n",
    "\n",
    "\n",
    "It can **optionally** include:\n",
    "- minimum and maximum values (error bars will represent the range of values)\n",
    "- standard deviation (error bars will represent one standard deviation)\n",
    "- additional numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our example data file consists of real polymeric SMILES, and is accompanied by fake median, mininum and maximum values. \\\n",
    "It also contains auxiliary data `extra_1`, `extra_2`, `extra_3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters optimization\n",
    "\n",
    "First, we have to optimize the architecture shape as well as its hyperparameters. For this, we have to define the hyperparameters space. In the SMILES-X, the hyperparameters that can be optimized are:\n",
    "\n",
    "**Geometry-related hyperparameters**\n",
    "- Embedding size\\\n",
    "    The dimensionality of the embedding\n",
    "- LSTM layer size\\\n",
    "    Number of units in the LSTM layer. As we use bidirectional LSTM, this number is internally multiplied by two during the run.\n",
    "- Time-distributed (TD) dense layer size\\\n",
    "    Number of units in the TD dense layer\n",
    "\n",
    "**Training-related hyperparameters**\n",
    "- Batch size\n",
    "- Learning rate\n",
    "   \n",
    "For each of the hyperparameter the bounds are defined as a **list of discrete values**.\\\n",
    "For the learning rate, we give a range of powers of 10.\\\n",
    "The typical bounds that can be used:\n",
    "\n",
    "    embed_bounds = [8, 16, 32, 64, 128, 256, 512]\n",
    "    lstm_bounds = [8, 16, 32, 64, 128, 256, 512]\n",
    "    tdense_bounds = [8, 16, 32, 64, 128, 256, 512]\n",
    "    bs_bounds = [8, 16, 32, 64, 128, 256, 512]\n",
    "    lr_bounds = [2, 2.1, 2.2, ..., 3.8, 3.9]\n",
    "  \n",
    "Note, that for the sake of demonstration, we run the code with narrower bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hyperparameters' bounds\n",
    "embed_bounds = [8, 16] # embedding size\n",
    "lstm_bounds = [8, 16] # number of units in the LSTM layer\n",
    "tdense_bounds = [8, 16] # number of units in the dense layer\n",
    "bs_bounds = [64, 128] # batch size\n",
    "lr_bounds = [2., 2.5, 3., 3.5] # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the user request, hyperparameters optimization can be performed in single- or two-steps fashion.\n",
    "\n",
    "Single-step optimizations implies optimization of all the given hyperparameters simultaneously via Bayesian optimization. Bayesian optimization is activate by setting `bayopt_mode='on'`.\n",
    "\n",
    "For the two-step optimization, SMILES-X implements zero-cost optimization of geometry-related hyperparameters. This allows to scan through the whole set of possible combinations of `[embed_bounds, lstm_bounds, tdense_bounds]` in a relatively short period of time. Trainless geometry optimization can be enable by setting `geomopt_mode='on'`. The remaining training-related hyperparameters are then optimized via Bayesian optimization, if it is activated. As trainless geometry optimization provides good and stable results, the two-step optimization is set by default.\n",
    "\n",
    "Hyperparameters optimization can be skipped altogether by setting both `bayopt_mode` and `geomopt_mode` to `'off'`. In this case, the user should provide referense values of the hyperparameters (`embed_ref`, `lstm_ref`, `dense_ref`, `bs_ref` and `lr_ref`). In case where no reference values are provided, default values are used. \n",
    "\n",
    "Note, that only the hyperparameters optimization will only optimize those hyperparameters, which have defined bounds, in accordance with the requested modes. The remaining onse will be set either to the user-defined or to the default values. For instance, it is possible to optimize only a part of geometry, or only the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SMILES-X training is performed by calling the `main` function. Here we define the inputs, optimization parameters, training parameters (number of epochs, number of independent runs, number of folds in k-fold cross validation, learning rate schedule and so on) and GPU setup.\n",
    "\n",
    "This will create a new path in the `./outputs` directory, corresponding to the indicated `data_name`. All the models, as well as auxiliary data such as scalers and logs, are stored there.\n",
    "\n",
    "Now you are ready to run the first SMILES-X model training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.main(data_smiles=data[['smiles1','smiles2']], # SMILES input\n",
    "          data_prop=data['median'], # Property of interest\n",
    "          data_err=data[['min', 'max']], # Error on the property\n",
    "          data_name=data_name,\n",
    "          data_units=data_units,\n",
    "          data_label=data_label,\n",
    "          smiles_concat=True,\n",
    "          geomopt_mode='off', # Zero-cost geometry optimization\n",
    "          bayopt_mode='off', # Bayesian optimization\n",
    "          train_mode='on', # Train\n",
    "          bs_bounds=bs_bounds,\n",
    "          lr_bounds=lr_bounds,\n",
    "          embed_bounds=embed_bounds,\n",
    "          lstm_bounds=lstm_bounds,\n",
    "          tdense_bounds=tdense_bounds,\n",
    "          k_fold_number=2, # Number of cross-validation splits\n",
    "          n_runs=3,# Number of runs per fold\n",
    "          check_smiles=True, # Verify SMILES validity via RDKit\n",
    "          augmentation=True, # Augment the data or not\n",
    "          bayopt_n_rounds=2,\n",
    "          bayopt_n_epochs=5,\n",
    "          bayopt_n_runs=2,\n",
    "          n_gpus=1,\n",
    "          n_epochs=10,\n",
    "          log_verbose=True, # To send print outs both to the file and console\n",
    "          train_verbose=True) # Show model training progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output data is organized in the following way:\n",
    "\n",
    "```\n",
    "./outputs\n",
    "└───Test\n",
    "    └───Augm\n",
    "        └───Train\n",
    "            │   Scores_Folds.csv # RMSE, MAE, R2 scores per fold\n",
    "            │   Scores_Final.csv # Final out-of-sample RMSE, MAE, R2 scores\n",
    "            │   Predictions.csv # Out-of-samples predictions for each SMILES\n",
    "            │   Train_yyyy-mm-dd_hh:mm:ss.log # Logs\n",
    "            │\n",
    "            └───Plots # Prediction vs Observation plots per fold and final out-of-sample\n",
    "            │   └───Prediction_vs_Observation\n",
    "            │   │   │   Test_PredvsTrue_Plot.png\n",
    "            │   │   │\n",
    "            │   │   └───Runs\n",
    "            │   │   │       Test_PredvsTrue_Plot_Fold_0_Run_0.png\n",
    "            │   │   │       Test_PredvsTrue_Plot_Fold_0_Run_1.png\n",
    "            │   │   │       Test_PredvsTrue_Plot_Fold_0_Run_2.png\n",
    "            │   │   │       Test_PredvsTrue_Plot_Fold_1_Run_0.png\n",
    "            │   │   │       ...\n",
    "            │   │   │\n",
    "            │   │   └───Folds\n",
    "            │   │           Test_PredvsTrue_Plot_Fold_0.png\n",
    "            │   │           Test_PredvsTrue_Plot_Fold_1.png\n",
    "            │   │           Test_PredvsTrue_Plot_Fold_2.png\n",
    "            │   │\n",
    "            │   └───Learning_Curves # Learning curves, one per run\n",
    "            │           Test_FitHistory_Fold_0_Run_0.png\n",
    "            │           Test_FitHistory_Fold_0_Run_1.png\n",
    "            │           Test_FitHistory_Fold_0_Run_2.png\n",
    "            │           Test_FitHistory_Fold_1_Run_0.png\n",
    "            │           ...\n",
    "            │\n",
    "            └───Models # One per run\n",
    "            │   │   Test_Model_Fold_0_Run_0.hdf5 \n",
    "            │   │   Test_Model_Fold_0_Run_1.hdf5 \n",
    "            │   │   Test_Model_Fold_0_Run_2.hdf5 \n",
    "            │   │   Test_Model_Fold_1_Run_0.hdf5\n",
    "            │   │   ...\n",
    "            │\n",
    "            └───Other # Auxiliary files\n",
    "                │   Test_Hyperparameters.csv # Hyperparameters used for training\n",
    "                │   Test_GeomScores.csv # Geometry optimization scores, sorted\n",
    "                │   Test_Vocabulary.txt # Vocabulary\n",
    "                │\n",
    "                └───Scalers\n",
    "                    │   Test_Scaler_Extra.pkl # Scaler for additional data input (if `data_extra` is provided)\n",
    "                    │   Test_Scaler_fold_0.pkl # Scaler for the outputs, one per fold\n",
    "                    │   Test_Scaler_fold_1.pkl # Scaler for the outputs, one per fold\n",
    "                    │   Test_Scaler_fold_2.pkl # Scaler for the outputs, one per fold\n",
    "```\n",
    "First, `Test` directory is created in `./outputs/`, which corresponds to the provided `data_name`. As the data augmentation has been requested, an `Augm` directory is created whithin `Test` one (otherwise it would be called `Can` for \"canonical\"). Finally, all the output files related to training are stored within the `Train` directory. \n",
    "\n",
    "First of, this is where per-SMILES predictions (`*_Predictions.csv`) and per-fold and final out-of-sample statistics (`*_Scores_Folds.csv` and `*_Scores_Final.csv`) are stored. The following several lines help to access them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "predictions = pd.read_csv(\"./outputs/{0}/Augm/Train/{0}_Predictions.csv\".format(data_name))\n",
    "scores_folds = pd.read_csv(\"./outputs/{0}/Augm/Train/{0}_Scores_Folds.csv\".format(data_name), header=[0,1,2],index_col=[0])\n",
    "scores_final = pd.read_csv(\"./outputs/{0}/Augm/Train/{0}_Scores_Final.csv\".format(data_name), header=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional numerical inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are going to traing a model with some additional data. These data will be injected in the model at the level between the attention and the output layer. In order to add non-linearity between the additional data, we insert 3 additional dense layers. We do not perform geometry optimization and set the architecture to some reference values. Data augmentation is set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main.main(data_smiles=data['smiles'], # SMILES input\n",
    "          data_prop=data['median'], # Property of interest\n",
    "          data_err=data[['min', 'max']], # Error on the property\n",
    "          data_extra=data[['extra_1', 'extra_2', 'extra_3']], # Auxiliary input data\n",
    "          data_name=data_name+'_EXTRA',\n",
    "          data_units=data_units,\n",
    "          data_label=data_label,\n",
    "          geomopt_mode='off', # Zero-cost geometry optimization\n",
    "          bayopt_mode='off', # Bayesian optimization\n",
    "          train_mode='on', # Train\n",
    "          bs_ref=128,\n",
    "          lr_ref=3.0,\n",
    "          embed_ref=8,\n",
    "          lstm_ref=8,\n",
    "          tdense_ref=8,\n",
    "          dense_depth=3,\n",
    "          k_fold_number=2, # Number of cross-validation splits\n",
    "          n_runs=3,# Number of runs per fold\n",
    "          check_smiles=True, # Verify SMILES validity via RDKit\n",
    "          augmentation=False, # Augment the data (only when `check_smiles` is set to `True`)\n",
    "          bayopt_n_rounds=2,\n",
    "          bayopt_n_epochs=5,\n",
    "          bayopt_n_runs=2,\n",
    "          n_gpus=1,\n",
    "          n_epochs=10,\n",
    "          log_verbose=True, # To send print outs both to the file and console\n",
    "          train_verbose=False) # Show model training progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference in the model structure printed by Keras.\n",
    "\n",
    "The outputs can be found in the `./outputs/Test_EXTRA/Can/Train/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use models\n",
    "\n",
    "Once the training is finished we have the access to the ensemble of models via `loadmodel` module. All that should be specified is the data_name and the augmentation flag used for training. The module will automatically collect models for each fold and run and return the ensemble together with auxiliary information, such as scalers or dataset statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the model for the simple version where the model is trained only with SMILES (no additional data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SMILESX import loadmodel\n",
    "\n",
    "# Load trained models once, use as many times as needed\n",
    "model = loadmodel.LoadModel(data_name = data_name,\n",
    "                            augment = True,\n",
    "                            gpu_ind = 0,\n",
    "                            return_attention = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the loaded model instance can be used for inference and/or interpretation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SMILESX import inference\n",
    "\n",
    "preds = inference.infer(model=model,\n",
    "                        data_smiles=['*OC(C)C(=O)*','CCC','C=O'],\n",
    "                        augment=True,\n",
    "                        check_smiles=True,\n",
    "                        log_verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In case of a classication task\n",
    "- Apply a threshold in order to reduce the predicted values to binary classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_asclass = (preds['mean'] > 0.5).astype(\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SMILESX import interpret\n",
    "\n",
    "inter = interpret.interpret(model=model,\n",
    "                            smiles=preds['SMILES'],\n",
    "                            pred=preds[['mean', 'sigma']],\n",
    "                            check_smiles=True,\n",
    "                            log_verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load models containing additional inputs. As the model is adjusted to take the additional inputs, they are also requested at the time of inference. For the same reason, temporal distance plot is not available for the models with additional inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = loadmodel.LoadModel(data_name = data_name+'_EXTRA',\n",
    "                            augment = False, # Since the model was trained without augmentation\n",
    "                            gpu_ind = 0,\n",
    "                            return_attention = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that data augmentation at the inference time can be requested regardless whether it was used for training or not. In this case, mean and standard deviation are computed over the augmentations and model ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = inference.infer(model=model,\n",
    "                        data_smiles=['*OC(C)C(=O)*','CCC','C=O'],\n",
    "                        data_extra=[[150,0.5,4], [87,0.2,15], [54,1.2,24]],\n",
    "                        augment=True, # Since we want to augment the data before making inference\n",
    "                        check_smiles=True,\n",
    "                        log_verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create fake true values for the molecules we passed for the prediction, to see the values on the interpretation 2D figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Fake ground truth values to show them 2D interpretation plots\n",
    "trues = pd.DataFrame([[400, 370, 415],\n",
    "                      [300, 280, 310],\n",
    "                      [200, 190, 205]])\n",
    "trues.columns = ['median', 'min', 'max']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the absence of temporal difference plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = interpret.interpret(model=model,\n",
    "                            smiles=preds['SMILES'],\n",
    "                            pred=preds[['mean', 'sigma']],\n",
    "                            true=trues['median'],\n",
    "                            true_err=trues[['min','max']],\n",
    "                            check_smiles=True,\n",
    "                            log_verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e4c38bed17e7e4388c5e46580947e1660cc2a36a8b64d8af04b9c333ab28a44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
